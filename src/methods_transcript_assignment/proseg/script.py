import os
import shutil
from pathlib import Path
import xarray as xr
import dask
import numpy as np
import pandas as pd
import anndata as ad
import spatialdata as sd
import sopa


## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
  'input_ist': 'resources_test/task_ist_preprocessing/mouse_brain_combined/raw_ist.zarr',
  'input_segmentation': 'resources_test/task_ist_preprocessing/mouse_brain_combined/segmentation.zarr',
  'transcripts_key': 'transcripts',
  'coordinate_system': 'global',
  'output': './temp/sopa_testing/proseg_transcripts.zarr',
  'voxel_layers': 4,
  'samples': 200,
  'burnin_samples': 200,
  'voxel_size': 1.0,
  'burnin_voxel_size': 2.0 
}
meta = {
  'name': 'proseg',
  'temp_dir': "/Users/habib/Projects/txsim_project/task_ist_preprocessing/temp/sopa",
  'cpus': 10
}
## VIASH END

TMP_DIR = Path(meta["temp_dir"] or "/tmp")
TMP_DIR.mkdir(parents=True, exist_ok=True)

# PROSEG_OUTPUT = TMP_DIR / "proseg_output.zarr"


# Read input
print('Reading input files', flush=True)
sdata = sd.read_zarr(par['input_ist'])
sdata_segm = sd.read_zarr(par['input_segmentation'])

# Check if coordinate system is available in input data
transcripts_coord_systems = sd.transformations.get_transformation(sdata[par["transcripts_key"]], get_all=True).keys()
assert par['coordinate_system'] in transcripts_coord_systems, f"Coordinate system '{par['coordinate_system']}' not found in input data."
segmentation_coord_systems = sd.transformations.get_transformation(sdata_segm["segmentation"], get_all=True).keys()
assert par['coordinate_system'] in segmentation_coord_systems, f"Coordinate system '{par['coordinate_system']}' not found in input data."

# Transform transcript coordinates to the coordinate system
print('Transforming transcripts coordinates', flush=True)
transcripts = sd.transform(sdata[par['transcripts_key']], to_coordinate_system=par['coordinate_system'])

# In case of a translation transformation of the segmentation (e.g. crop of the data), we need to adjust the transcript coordinates
trans = sd.transformations.get_transformation(sdata_segm["segmentation"], get_all=True)[par['coordinate_system']].inverse()
transcripts = sd.transform(transcripts, trans, par['coordinate_system'])

#load prior segmentation image
if isinstance(sdata_segm["segmentation"], xr.DataTree):
    label_image = sdata_segm["segmentation"]["scale0"].image.to_numpy() 
else:
    label_image = sdata_segm["segmentation"].to_numpy()
    
# assign transcripts to cells based on x,y coords and segmentation image
y_coords = transcripts.y.compute().to_numpy(dtype=np.int64)
x_coords = transcripts.x.compute().to_numpy(dtype=np.int64)
cell_id_dask_series = dask.dataframe.from_dask_array(
    dask.array.from_array(
        label_image[y_coords, x_coords], chunks=tuple(sdata[par['transcripts_key']].map_partitions(len).compute())
    ), 
    index=sdata[par['transcripts_key']].index
)
sdata[par['transcripts_key']]["cell_id"] = cell_id_dask_series


### Run Proseg with sopa

# Create reduced sdata
print("Creating sopa SpatialData object")
sdata_sopa = sd.SpatialData(
    points={
        "transcripts": sdata[par['transcripts_key']]
    },
)

# Make transcript patches
sopa.make_transcript_patches(sdata_sopa, patch_width=2000, patch_overlap=50, prior_shapes_key="cell_id")
sdata_sopa['transcripts'].attrs['spatialdata_attrs'] = {}
sdata_sopa['transcripts'].attrs['spatialdata_attrs']['feature_key'] = 'feature_name'

#TODO add command suffix
n_threads = meta['cpus'] or os.cpu_count()
n_threads = max(n_threads-2, 1)

print(f'Running Proseg with {n_threads} threads', flush=True)

command_suffix = (
    f'''--nthreads {n_threads} '''
# these should not need to be changed
    f'''--ncomponents 10 '''
    # f'''--no-diffusion ''' # should be off by default (?)
    f'''--diffusion-probability 0.2 '''
    f'''--diffusion-sigma-far 4 '''
    f'''--diffusion-sigma-near 1 '''
    f'''--nuclear-reassignment-prob 0.2 '''
    f'''--cell-compactness 0.03 '''
# these can be changed as arguments
    f'''--voxel-layers {par['voxel_layers']} '''
    f'''--samples {par['samples']} '''
    f'''--burnin-samples {par['burnin_samples']} '''
    f'''--voxel-size  {par['voxel_size']} '''
    f'''--burnin-voxel-size {par['burnin_voxel_size']} '''
)
print('Using parameters:' + command_suffix.replace('--', '\n\t'))
sopa.segmentation.proseg(sdata_sopa, key_added='proseg_boundaries', command_line_suffix=command_suffix)


sopa.spatial.assign_transcript_to_cell(
    sdata_sopa,
    points_key="transcripts",
    shapes_key="proseg_boundaries",
    key_added="cell_id",
    unassigned_value=0
)

# Create objects for cells table
print('Creating objects for cells table', flush=True)
#create new .obs for cells based on the segmentation output (corresponding with the transcripts 'cell_id')
unique_cells = np.unique(sdata_sopa["transcripts"]["cell_id"])

# check if a '0' (noise/background) cell is in cell_id and remove
zero_idx = np.where(unique_cells == 0)
if len(zero_idx[0]): unique_cells=np.delete(unique_cells, zero_idx[0][0])

#transform into pandas series and check
cell_id_col = pd.Series(unique_cells, name='cell_id', index=unique_cells)
assert 0 not in cell_id_col, "Found '0' in cell_id column of assingment output cell matrix"


# Create transcripts only sdata
print('Subsetting to transcripts cell id data', flush=True)
sdata_transcripts_only = sd.SpatialData(
    points={
        "transcripts": sdata_sopa['transcripts']
    },
    tables={
        "table": ad.AnnData(
          obs=pd.DataFrame(cell_id_col),
          var=sdata.tables["table"].var[[]]
        )
    }
)

# Write output
print('Write transcripts with cell ids', flush=True)
if os.path.exists(par["output"]):
    shutil.rmtree(par["output"])
    
sdata_transcripts_only.write(par['output'])
