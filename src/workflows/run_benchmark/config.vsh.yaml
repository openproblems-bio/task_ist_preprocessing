name: run_benchmark
namespace: workflows

argument_groups:
  - name: Inputs
    arguments:
      - name: "--input_sc"
        __merge__: /src/api/file_singlecell.yaml
        type: file
        direction: input
        required: true
      - name: "--input_sp"
        __merge__: /src/api/file_spatialdata.yaml
        type: file
        direction: input
        required: true
  - name: Outputs
    arguments:
      - name: "--output_scores"
        type: file
        required: true
        direction: output
        description: A yaml file containing the scores of each of the methods
        default: score_uns.yaml
      - name: "--output_method_configs"
        type: file
        required: true
        direction: output
        default: method_configs.yaml
      - name: "--output_metric_configs"
        type: file
        required: true
        direction: output
        default: metric_configs.yaml
      - name: "--output_dataset_info"
        type: file
        required: true
        direction: output
        default: dataset_uns.yaml
      - name: "--output_task_info"
        type: file
        required: true
        direction: output
        default: task_info.yaml
  - name: Methods
    arguments:
      - name: "--method_ids"
        type: string
        multiple: true
        description: A list of method ids to run. If not specified, all methods will be run.

resources:
  - type: nextflow_script
    path: main.nf
    entrypoint: run_wf
  - type: file
    path: /_viash.yaml

dependencies:
  # - name: common/check_dataset_schema
  #   repository: openproblems-v2
  # - name: common/extract_metadata
  #   repository: openproblems-v2
  # - name: control_methods/true_labels
  # - name: methods/logistic_regression
  # - name: metrics/accuracy

runners:
  - type: nextflow
